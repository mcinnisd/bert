{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 595 HW3: Parts 1-3 Building and Pre-Training BERT from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Bring in PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# optimizer transfo\n",
    "\n",
    "# Most of the examples have typing on the signatures for readability\n",
    "from typing import Optional, Callable, List, Tuple\n",
    "from copy import deepcopy\n",
    "# For data loading\n",
    "from torch.utils.data import Dataset, IterableDataset, TensorDataset, DataLoader\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "import bz2\n",
    "import wandb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For progress and timing\n",
    "from tqdm.auto import tqdm, trange\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Tokenization\n",
    "\n",
    "To start with, we'll load in one of the tokenizers for the original BERT from the `tokenizers` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hello', ',', 'i', 'am', 'learning', 'to', 'token', '##ize', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "#                     TODO: YOUR CODE HERE                     #\n",
    "#\n",
    "#  1. Create a new BertWordPieceTokenizer using the specified vocab.txt file in the homework. \n",
    "#     Make sure that all tokens are lowed cased.\n",
    "#\n",
    "################################################################\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\"vocab.txt\", lowercase=True)\n",
    "\n",
    "# Test the tokenizer\n",
    "print(tokenizer.encode(\"Hello, I am learning to tokenize\").tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'my', 'first', 'test', 'sentence', \"'\", 's', 'words', '.', 'i', 'am', 'programming', '.', 'un', '##con', '##ditional', '##ly', ',', 'comfortable', ';', 'professional', '.', 'non', '##gram', '##matic', '##al', 'sentence', '.', '.', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"This is my first test sentence's words. I am programming. Unconditionally, comfortable; professional. nongrammatical sentence... \"\n",
    "\n",
    "print(tokenizer.encode(sentence1).tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Building a Transformer Encoder and BERT\n",
    "\n",
    "The BERT model is a pre-trained transformer network. Creating a small BERT model will take two big piecse (1) building the transformer itself and (2) writing the code that incorporates the tranformer into BERT and has it set up for training. We'll try to simplify building this by thinking of the different pieces as building blocks that we can put together. Remember, all neural networks are _functions_ and you can compose functions together to make a new function. \n",
    "\n",
    "Let's take a look at the overall diagram for Transformers/BERT:\n",
    "![The diagram of the transformer newtork](https://devopedia.org/images/article/235/5113.1573652896.png)\n",
    "The trickiest part is the left where we need to deal with the scaled dot-product attention. You've already seen attention though in Homework 2, so some of this should be familiar. \n",
    "\n",
    "You'll implement the following pieces to put it all together.\n",
    "\n",
    "Steps:\n",
    "1. Embedding: BERT will learn word embeddings that are similar to word2vec's _but_ also incorporate the position of the embedding in the sequence\n",
    "2. Multi-headed Attention: The core part of the network that learns how much each token should pay attention to all other tokens\n",
    "3. A Feed-forward Layer: The layer that transformers the attention-combined representations\n",
    "4. The Transformer Encoder: The unified transformer network that combines attention with the feed-forward layer\n",
    "5. A BERT Classification Layer: The classificiation part of BERT\n",
    "6. A BERT Masked Language Modeling (MLM) Layer: The part of BERT that deals with MLM training\n",
    "7. The overall BERT model: The final BERT model architecture that supports both MLM and Classification\n",
    "\n",
    "Feel free to read the instructions for all steps in Part 2 before getting started to see how the pieces might fit together. To get everything working, you'll need all parts, which build on each other, so we recommend starting with the first and moving on from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1: Embedding Layer\n",
    "BERT's input embeddings are normally the sum of three embeddings:\n",
    "- Token Embeddings: The input token embeddings\n",
    "- Position Embeddings: The position of the token in the sequence\n",
    "- Token Type Embeddings: The segment (sentence) the token belongs to\n",
    "\n",
    "The second piece helps BERT learn to distinguish that the same token is in different positions. Remember the attention mechanism works independently of where each of the tokens are; without positional information added to the word embedding, the model can't distinguish between words in different orders!\n",
    "\n",
    "The third piece was designed for the Next Sentence Prediciton (NSP) task during pre-training. Here, two sentences are provided as input with the special `[SEP]` token between them. The NSP task is a classifiction task based on whether the two sentences did or did not actually follow each other. Just like in word2vec, we would sample random sentences as not-next. The hope for this pretraining task was that it would help BERT learn discourse coherence. However, some later works have shown NSP doesn't actually help that much and training time is probably better spent on doing more MLM, so some more advanced models dropped this.\n",
    "\n",
    "For simplicity, in Homework 3, you only need to deal with token embeddings and positional embeddings, but do not need to deal with token-type embeddings.\n",
    "\n",
    "**NOTE:** When talking about embeddings, people (and these instructions) will talk about various things being embedded, e.g., words, tokens, wordpieces, subwords, etc. In practice, these are all based on whatever the tokenizer is producing, and BERT (and you) is agnostic to what is actually being embedded. When the instructions talk about \"token embeddings\" this are still just the output of the BERT WordPiece tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_dim: int, \n",
    "                 hidden_dim: int = 768, \n",
    "                 padding_idx: int = 0, \n",
    "                 max_seq_length: int = 512):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        '''\n",
    "        Initialize the Embedding Layers\n",
    "        '''\n",
    "\n",
    "        ################################################################################\n",
    "        #                             TODO: YOUR CODE HERE                             #\n",
    "        #                                                                              #\n",
    "        #  1. Create two Embedding objects for the words and the positions.            #\n",
    "        #     For the word embeddings keep track of which index is the padding index.  #        \n",
    "        #                                                                              #\n",
    "        ################################################################################\n",
    "        self.word_embeddings = nn.Embedding(vocab_dim, hidden_dim, padding_idx=padding_idx)\n",
    "        self.positional_embeddings = nn.Embedding(max_seq_length, hidden_dim)\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor, \n",
    "                ) -> torch.Tensor:\n",
    "        \n",
    "        '''\n",
    "        Define the forward pass of the Embedding Layers\n",
    "        '''\n",
    "        \n",
    "        ############################################################################\n",
    "        #                               TODO: YOUR CODE HERE                       #\n",
    "        #                                                                          #\n",
    "        # 1. Look up the relevant token embeddings from the word_embeddings layer  #\n",
    "        # 2. Return the sum of the token embeddings and the positional embeddings  #\n",
    "        #                                                                          #\n",
    "        ############################################################################\n",
    "\n",
    "        B = token_ids.size(0)\n",
    "        T = token_ids.size(1)\n",
    "        # print(f\"token ids: {token_ids.shape}\")\n",
    "\n",
    "        # print(f\"Batch size: {B}\")\n",
    "        # print(f\"Seq len: {T}\")\n",
    "\n",
    "        positions = torch.arange(T, device=token_ids.device, dtype=torch.long)\n",
    "        #positions = positions.unsqueeze(0).expand(B, -1) # for batch\n",
    "        # print(f\"positions: {positions}\")\n",
    "      \n",
    "        position_embeddings = self.positional_embeddings(positions).unsqueeze(0)\n",
    "        token_embeddings = self.word_embeddings(token_ids)\n",
    "\n",
    "        # print(f\"Positional embeddings: {position_embeddings.shape}\")\n",
    "        # print(f\"token embeddings: {token_embeddings.shape}\")\n",
    "\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2: Multi-Head Attention\n",
    "\n",
    "This is the trickiest part of the homework where we implement the attention part of the transformer. Let's take a look at the attention:\n",
    "\n",
    "![The attention network](https://www.tutorialexample.com/wp-content/uploads/2021/03/The-structure-of-Multi-Head-Attention.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4, 4])\n",
      "vemb torch.Size([10, 4, 7])\n",
      "torch.Size([10, 4, 7])\n"
     ]
    }
   ],
   "source": [
    "# Optional TODO: Try implementing attention with some embeddings to get a sense of the core parts\n",
    "# Note that this is not a complete implementation of the attention mechanism in the transformer,\n",
    "# which also includes some scaling and masking operations as well as dealing with multiple heads.\n",
    "\n",
    "# Generate some Q, K, V embeddings for a batch of 10 sequences of length 4 with embedding size 7\n",
    "q_emb = torch.randn(10, 4, 7)\n",
    "k_emb = torch.randn(10, 4, 7)\n",
    "v_emb = torch.randn(10, 4, 7)\n",
    "\n",
    "# Start by computing the dot product of the Q and K embeddings\n",
    "dot_prod = (q_emb @ k_emb.transpose(-1, -2)) \n",
    "\n",
    "# Then apply a softmax to get the attention weights\n",
    "attn = nn.functional.softmax(dot_prod, dim=-1)\n",
    "\n",
    "# This should be torch.Size([10, 4, 4]) --- i.e., how much each word (4 words) pays attention to each other word\n",
    "print(attn.shape)\n",
    "print(f\"vemb {v_emb.shape}\")\n",
    "\n",
    "# Now compute the weighted words by multiplying the attention weights by the V embeddings\n",
    "weighted_output = attn @ v_emb\n",
    "\n",
    "# This should be torch.Size([10, 4, 7]) --- i.e., the same shape as our inputs, but the embedings are weighted combinations of the V embeddings!\n",
    "print(weighted_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "tensor([[[7, 3, 7, 8],\n",
      "         [1, 5, 0, 2],\n",
      "         [9, 4, 2, 3]],\n",
      "\n",
      "        [[7, 7, 9, 0],\n",
      "         [6, 6, 5, 9],\n",
      "         [9, 3, 4, 7]]])\n"
     ]
    }
   ],
   "source": [
    "# Optional TODO: How to implicitly represent attention weights in a single layer\n",
    "\n",
    "# Let's start with a single projection of the input embedding for Q for a sequence of 3 words with embedding size 8\n",
    "# NOTE: we'll use randint here to make it easier to see the reshaping when we print the tensors\n",
    "q_emb = torch.randint(10, (3, 8))\n",
    "# print(q_emb)\n",
    "# If we have two attention heads we can reshape this tensor so that the first dimension is the number of heads\n",
    "q_s = q_emb.shape\n",
    "# Should be torch.Size([3, 8])\n",
    "# print(q_s)\n",
    "n_heads = 2\n",
    "multihead_q_emb = q_emb.view(n_heads, q_s[0], q_s[1]//n_heads)\n",
    "\n",
    "# Should be torch.Size([2, 3, 4])\n",
    "print(multihead_q_emb.shape)\n",
    "\n",
    "# Check that the reshaping is correct by looking at which values when where\n",
    "print(multihead_q_emb)\n",
    "\n",
    "# Remember: Each head has its own attention! So we'll need to use this kind of reshaping for K and V so we can evantually compute\n",
    "# the per-head specifici attention weights. \n",
    "\n",
    "# Optional TODO: try creating the K projection and re-shaping it and then the calculate its attention weights using the logic from the above cell\n",
    "# Note that you'll now have another dimension in the tensor! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        '''\n",
    "        Arguments:\n",
    "        hidden_size: The total size of the hidden layer (across all heads)\n",
    "        num_heads: The number of attention heads to use\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        '''\n",
    "        Initialize the Multi-Headed Attention Layer\n",
    "        '''\n",
    "\n",
    "        ###########################################################################################################################\n",
    "        #                                                     TODO: YOUR CODE HERE                                                #\n",
    "        #                                                                                                                         #\n",
    "        # 1. Figure out how many dimensions each head should have                                                                 #\n",
    "        # 2. Create linear layers to turn the input embeddings into the query, key, and value projections                         #\n",
    "        # 3. Calculate the scale factor (1 / sqrt(per-head embedding size))                                                       #\n",
    "        #                                                                                                                         #                                            \n",
    "        # NOTE: Each of the Q, K, V projections represents the projections of *each* of the heads as one long sequence.           #\n",
    "        #       Each of the layers is implicitly representing each head in different parts of its dimensions. E.g., if you        #\n",
    "        #       have 4 heads and 16 dimensions, the first 4 dimensions are the first head, the second 4 dimensions are            #\n",
    "        #       the second head, etc.                                                                                             #                        \n",
    "        ###########################################################################################################################\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size // num_heads\n",
    "\n",
    "        self.query_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.scale_factor = 1.0 / (self.head_size ** 0.5)\n",
    "\n",
    "\n",
    "    def forward(self, embeds: torch.Tensor, \n",
    "                mask: Optional[torch.Tensor] = None\n",
    "                ) -> torch.Tensor:\n",
    "        '''\n",
    "        Arguments:\n",
    "        embeds: The input embeddings to compute the attention over\n",
    "        mask: A boolean mask of which tokens are valid to use for computing attention (see collate below)\n",
    "        '''\n",
    "\n",
    "        #####################################################################################################################################################\n",
    "        #                                                   TODO: YOUR CODE HERE                                                                            #\n",
    "        #                                                                                                                                                   #\n",
    "        # This is the hard part of the assignment where you'll need to implement the multi-headed attention mechanism.                                      #\n",
    "        # The cell above has the core logic for the attention mechanism to get you started with getting the tensor                                          #\n",
    "        # shapes lined up correctly. We recommend working through that manually with some small examples to get a sense                                     #\n",
    "        # of what is happening at each step.                                                                                                                #\n",
    "        #                                                                                                                                                   #\n",
    "        # One of the key parts to work through is how to handle the shapes of the different embeddings. For attention to work                               #\n",
    "        # and be efficient, we'll only need to do a few matrix multiplications and a softmax. The key is to figure out how to                               #\n",
    "        # do this by getting the tensors into the right shapes. We strongly recommend trying to write comments at each step                                 #\n",
    "        # That describe the shape of the tensors at each step in terms of what each representings.   This will help you understand                          #\n",
    "        # what is happening and debug.                                                                                                                      #\n",
    "        #                                                                                                                                                   #\n",
    "        # We recommend using notation like the following which you'll also see in papers and blogs:                                                         #\n",
    "        #  - B: Batch size                                                                                                                                  #\n",
    "        #  - H: Number of heads                                                                                                                             #\n",
    "        #  - T: Sequence length                                                                                                                             #\n",
    "        #  - D: Embedding size                                                                                                                              #\n",
    "        #                                                                                                                                                   #\n",
    "        # 1. Figure out what are the dimensions of the input embeddings and which dimensions                                                                #\n",
    "        #    represent what (e.g., the batch size, sequence length, etc.)                                                                                   #\n",
    "        # 2. Project the input embeddings into the Q, K, and V spaces                                                                                       #\n",
    "        # 3. Compute the attention weights from the Q and K projections (be sure to scale the dot product by the scale factor!)                             #\n",
    "        # 4. *If there is a mask*, apply the mask to the attention weights where masked values are set to -inf                                              #\n",
    "        # 5. Compute the weighted sum of the V embeddings using the attention weights                                                                       #\n",
    "        # 6. Return the re-weighted output values *and* the attention weights in the shape (Batch, Heads, SeqLen, SeqLen)                                   #\n",
    "        #    We'll use the attention weights for visualization later                                                                                        #\n",
    "        #                                                                                                                                                   #\n",
    "        # NOTE: when we say dimension, we're referring to how many axes are in a tensor. E.g., a vector                                                     #\n",
    "        #       is a one-dimensional tensor, a matrix is a two-dimensional tensor, etc. Each dimension has a size too (number of components),               #\n",
    "        #       e.g., a 3x4 tensor has 2 dimensions, where the first has 3 elements/components and the second has 4 elements/components.                    #\n",
    "        # NOTE: You will probably want to use the .view() method to reshape the input embeddings with respect to the number of heads                        #\n",
    "        #       so that you can get a tensor where one dimension corresponds to each head                                                                   #\n",
    "        # NOTE: You may want to use the .transpose() method to swap dimensions (e.g., to move the heads dimension to the front)                             #\n",
    "        # NOTE: Check out masked_fill for applying the mask to the attention weights                                                                        #\n",
    "        # NOTE: You will not need to concatenate anything in practice because the Q, K, V projections already represent concatenated head-specific values   #\n",
    "        #    in the right dimensions.                                                                                                                       #\n",
    "        # HINT: You can reshape any tensor with view to \"add a dimension\" to it.                                                                            #\n",
    "        #    E.g., if you have a tensor with shape (B, T, D) and you want to add a dimension to the front, you can do tensor.view(B, 1, T, D)               #\n",
    "        #####################################################################################################################################################\n",
    "        \n",
    "        B, T, D = embeds.size()\n",
    "        # print(f\"Batch: {B} sequence: {T} embedding: {D}\")\n",
    "\n",
    "        H = self.num_heads\n",
    "        # H = D // self.head_size\n",
    "        # print(f\"Heads: {H}\")\n",
    "\n",
    "        # print(f\"Query proj size: {self.query_proj.weight.shape}\")\n",
    "        # print(f\"Key proj size: {self.key_proj.weight.shape}\")\n",
    "        # print(f\"Value proj size {self.value_proj.weight.shape}\")\n",
    "\n",
    "\n",
    "        query = self.query_proj(embeds).view(B, T, H, self.head_size).transpose(1, 2)\n",
    "        key = self.key_proj(embeds).view(B, T, H, self.head_size).transpose(1, 2)\n",
    "        value = self.value_proj(embeds).view(B, T, H, self.head_size).transpose(1, 2)\n",
    "\n",
    "        # query = self.query_proj(embeds).view(B, H, T, self.head_size)\n",
    "        # key = self.key_proj(embeds).view(B, H, T, self.head_size)\n",
    "        # value = self.value_proj(embeds).view(B, H, T, self.head_size)\n",
    "\n",
    "        # print(f\"Query size: {query.size()}\")\n",
    "        # print(f\"Key size: {key.size()}\")\n",
    "        # print(f\"Value size {value.size()}\")\n",
    "\n",
    "        # attention_scores = nn.Linear(query, key.transpose(-1, -2)) * self.scale_factor\n",
    "        attention_scores = (query @ key.transpose(-1, -2)) * self.scale_factor\n",
    "        # print(f\"Attention score size: {attention_scores.size()}\")\n",
    "\n",
    "        if mask is not None:\n",
    "            expanded_mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            # print(f\"explanded mask: {expanded_mask}\")\n",
    "            attention_scores = attention_scores.masked_fill(expanded_mask == 0, float('-inf'))\n",
    "            # print(f\"attention: {attention_scores}\")\n",
    "\n",
    "            # print(f\"attn score after mask: {attention_scores.shape}\")\n",
    "\n",
    "        attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # print(f\"Attention weights size: {attention_weights.size()}\")\n",
    "        \n",
    "        # weighted_sum = nn.Linear(attention_weights, value)\n",
    "        weighted_sum = attention_weights @ value\n",
    "        weighted_sum = weighted_sum.transpose(1,2).contiguous()\n",
    "        # print(f\"weighted sum: {weighted_sum.size()}\")\n",
    "        output = self.output_layer(weighted_sum.view(B, T, -1))\n",
    "\n",
    "        # print(f\"Batch: {B} Heads: {H} Sequence: {T}\")\n",
    "\n",
    "        return output, attention_weights\n",
    "        \n",
    "        # pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.3 Define the Feed-Forward Layer\n",
    "\n",
    "The feed forward layer is a simple two-layer feed forward network (FFN) with an activation function between the layers. This network usually follows the multi-headed attention output that allows its content representation to be tranformed and aggregated. Below, we'll make a function that return this network using `nn.Sequential` which takes in a tuple or list of layers and activation functions where any input is passed through each function in order and then the output is returned.\n",
    "\n",
    "In a tranfsormer, typically the FFN is wider than the embedding size, usally with an expansio factor of 4. This increased number of neurons allows the model to capture more interactions between dimensions of the embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_layer(\n",
    "    hidden_size: int, \n",
    "    feed_forward_size: Optional[int] = None, \n",
    "    activation: nn.Module = nn.GELU()\n",
    "):\n",
    "    '''\n",
    "    Arguments:\n",
    "      - hidden_size: The size of the input and output of the feed forward layer. \n",
    "      - feed_forward_size: The size of the hidden layer in the feed forward network. If None, defaults to 4 * hidden_size. This size\n",
    "        specifies the size of the middle layer in the feed forward network.\n",
    "      - activation: The activation function to use in the feed forward network\n",
    "\n",
    "    Returns: \n",
    "    '''\n",
    "    ################################################################\n",
    "    #                     TODO: YOUR CODE HERE                     #\n",
    "    # Implement the feed forward layer as described in the slides  #\n",
    "    # The feed forward layer is a simple three-layer neural network#\n",
    "    # with an activation function.                                 #\n",
    "    #                                                              #\n",
    "    # NOTE: It maps from hidden_size to feed_forward_size and then #\n",
    "    #       back to hidden_size.                                   #\n",
    "    ################################################################\n",
    "\n",
    "    feed_forward_size = feed_forward_size or 4 * hidden_size\n",
    "\n",
    "    feed_forward = nn.Sequential(\n",
    "        nn.Linear(hidden_size, feed_forward_size),\n",
    "        activation,\n",
    "        nn.Linear(feed_forward_size, hidden_size)\n",
    "    )\n",
    "\n",
    "    return feed_forward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.4 Building a Transformer Block as an Encoder Layer\n",
    "\n",
    "Let's finish putting together our tranformer pieces into a single network that (1) computes the self-attention to get contextualized word representations and (2) passes those representations through our feed-forward neural network layers. \n",
    "\n",
    "During training, we'll also add some probability of using [dropout](https://machinelearningmastery.com/using-dropout-regularization-in-pytorch-models/) which is also implemented in pytorch. Dropout will randomly zero-out some values when training so the model learns to be robust to the value of any one neuron. In practice, when you switch between `train()` and `eval()`, under the hood, this will turn on/off things like dropout automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 256, # NOTE: normally 768, but keep it small for homework\n",
    "        num_heads: int = 12,\n",
    "        dropout: float = 0.1,\n",
    "        activation: nn.Module = nn.GELU(),\n",
    "        feed_forward_size: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        # Now we can put it all together to create one layer of the    #\n",
    "        # transformer encoder.                                         #\n",
    "        #                                                              #\n",
    "        # 1. Create a MultiHeadedAttention layer with the specified    #\n",
    "        #    number of heads.                                          #\n",
    "        # 2. Create a feed forward layer with the specified activation #\n",
    "        #    function.                                                 #\n",
    "        # 3. Save the hidden_size, dropout, and feed_forward_size      #\n",
    "        #    as attributes.                                            #\n",
    "        # 4. Define a forward method that takes in an input tensor     #\n",
    "        #    and an optional mask tensor and returns the output tensor #\n",
    "        #    and the attention weights that go through the multi-      #\n",
    "        #    headed attention layer and the feed forward layer.        #\n",
    "        ################################################################\n",
    "\n",
    "        self.multihead_attention = MultiHeadedAttention(hidden_size, num_heads)\n",
    "        # self.attention_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.feed_forward = feed_forward_layer(hidden_size, feed_forward_size, activation)\n",
    "        # self.feed_forward_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.feed_forward_size = feed_forward_size\n",
    "\n",
    "    def maybe_dropout(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        # Implement the dropout layer to be used in the forward pass   #\n",
    "        # of the transformer encoder layer (if dropout was specified)  #\n",
    "        ################################################################\n",
    "\n",
    "        if self.dropout.p > 0:\n",
    "            return self.dropout(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        '''\n",
    "        Returns the output of the transformer encoder layer and the attention weights from the self-attention layer\n",
    "        '''\n",
    "        \n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        #\n",
    "        # 1. Pass the input tensor through the self-attention layer    \n",
    "        # 2. Call maybe_dropout on the output of the self-attention\n",
    "        # 3. Pass the output of the self-attention through the feed forward network\n",
    "        # 4. return the output of the feed forward network and the attention weights\n",
    "        #\n",
    "        ################################################################\n",
    "\n",
    "       \n",
    "\n",
    "        # print(f\"x shape: {x.shape}\")\n",
    "        # print(f\"mask shape; {mask.shape}\")\n",
    "\n",
    "        attention_output, attention_weights = self.multihead_attention(x, mask)\n",
    "        # print(f\"attn_out: {attention_output.shape}\")\n",
    "        # print(f\"atten_weight: {attention_weights.shape}\")\n",
    "\n",
    "        # residual connection: y = x + dropout(attention(x))\n",
    "        attention_output = x + self.maybe_dropout(attention_output)\n",
    "        # attention_output = self.attention_norm(x + self.maybe_dropout(attention_output))\n",
    "        # print(f\"attn_out2: {attention_output.shape}\")\n",
    "\n",
    "        ff_output = self.feed_forward(attention_output)\n",
    "        # print(f\"ff: {ff_output.shape}\")\n",
    "\n",
    "        # residual connection: z = y + dropout(feedforward(y))\n",
    "        output = attention_output + self.maybe_dropout(ff_output)\n",
    "        # output = self.feed_forward_norm(attention_output + self.maybe_dropout(ff_output))\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.5: Masked Language Modeling Head\n",
    "\n",
    "A BERT model usually comes with multiple \"heads\" that are used for different tasks. One of these heads is used for the masked language model (MLM) task during pretraining. \n",
    "        \n",
    "In practice, a \"head\" is just a linear layer that maps the hidden size to the output size. This linear layer allows the model to adapt the token representations to a particular task, while keeping the core transformer model parameters the same across tasks. A model could have multiple heads for different tasks.  In our implementation, we'll have two heads, one for the masked language modeling task and a second for classification.\n",
    "\n",
    "The MLM head maps the contextualized token embedding to the vocabulary, so if we have some embedding $e_i$, we're learning a weight matrix $W_{mlm}$ of size $|e_i| \\times |V|$. In BERT pre-training, this weight matrix is said to be _tied_ to the input embeddings; in practice, that means we use the same weights from the `Embedding` (!), except this time the weights are used as a linear layer! (NOTE: This means you're not defining a separate linear layer). You might also see this called \"parameter sharing\" where the same parameters (i.e., weights) are used in different parts of the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMHead(nn.Module):\n",
    "    def __init__(self, word_embeddings: nn.Embedding):\n",
    "        '''\n",
    "        Arguments:\n",
    "            word_embeddings: The word embeddings to use for the prediction\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: The input tensor to the MLM head containing a batch of sequences of\n",
    "           contextualized word embeddings (activations from the transformer encoder \n",
    "           layers)\n",
    "        '''\n",
    "        \n",
    "        ################################################################\n",
    "        #                 TODO: YOUR CODE HERE                         #\n",
    "        #                                                              #\n",
    "        # The MLM head is used to predict the original token from      #\n",
    "        # the masked token. The prediction is over the whole \n",
    "        # vocabulary so we'll need an activation. To make this work,\n",
    "        # we'll generate a tensor the length of the vocabulary size\n",
    "        # that we can push through a softmax to get the probabilities\n",
    "        # of each token being present.\n",
    "        #                                                              #\n",
    "        # NOTE: The head should not have an activation function.       #\n",
    "        # NOTE: The head should be tied to the input embeddings (i.e., #\n",
    "        #       the head should map the embeddings to the vocab size). #\n",
    "        #       In other words, the MLM head directly predicts the     #\n",
    "        #       token from the learned embeddings instead of the       #\n",
    "        #       last hidden states.                                    #\n",
    "        # HINT: You can get the tensor of the word embeddings to use   #\n",
    "        #       for prediction from the word_embeddings object.        #\n",
    "        # HINT: Desipte all this writing, this function is only a \n",
    "        #       single line of code.                                   #\n",
    "        ################################################################\n",
    "\n",
    "        return x @ self.word_embeddings.weight.transpose(0, 1)\n",
    "    \n",
    "        # return nn.Linear(x, self.word_embeddings.weight.transpose(0, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.6: Classification Pooler Head\n",
    "\n",
    "When we pre-train, we'll learn a special embedding called `[CLS]` that is the first embedding in any sequences and loosely approximates the overall meaning/semantics of the input text. Frequently, when we fine-tune a BERT classifier, we're using this `[CLS]` token as the summary of the input and updating the weights accordingly. \n",
    "\n",
    "Here, we'll create a network that will add a `Linear` layer on top of the `[CLS]` token which can be fine-tuned to do classification later. This linear layer allows use to adapt/project the `[CLS]` to a representation more suitable for classification.  This is the classification head of BERT.\n",
    "\n",
    "We'll use this network later when defining the `BERT` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooler(nn.Module):\n",
    "    def __init__(self, hidden_size: int = 768):\n",
    "        super().__init__()\n",
    "\n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        # The BERT model usually uses the first token of the sequence  #\n",
    "        # to represent the entire sequence (this is the [CLS] token).  #\n",
    "        #                                                              #\n",
    "        # The pooler layer is a simple linear layer that maps the      #\n",
    "        # representation of the [CLS] token to the hidden size.        #\n",
    "        #                                                              #\n",
    "        # This pooled representation is used as the input to the       #\n",
    "        # classification layer defined in the later cell.              #\n",
    "        ################################################################\n",
    "\n",
    "        self.pooler_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        #\n",
    "        # 1. Pass the [CLS] embedding through the dense layer \n",
    "        # 2. Pass the output of the dense layer through the activation\n",
    "        # 3. Return the output of the activation\n",
    "        #\n",
    "        # OPTIONAL TODO: One other way you can represent the contents of \n",
    "        #             the entire sequence is to use the mean of all the non-special\n",
    "        #             token embeddings (also known as \"mean pooling\"). \n",
    "        #             You can try implementing that here and add a flag to the\n",
    "        #             Pooler __init__ function to switch between the two approaches.\n",
    "        #\n",
    "        ################################################################\n",
    "        cls = x[:, 0]\n",
    "\n",
    "        # print(f\"CLS {cls}\")\n",
    "\n",
    "        pooled_output = self.pooler_layer(cls)\n",
    "\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.7: The BERT network!\n",
    "\n",
    "Finally! We have arrived at putting all the pieces together into a single neural network. Our BERT model will have the main attention plus feed-forward network components and then two heads: one for MLM and one for Classification. At model creation time, we'll specify which \"mode\" the model should be in (MLM or classification).\n",
    "\n",
    "This implementation is a good reminder that neural networks (e.g., `nn.Module`) are just functions on inputs. We can compose and stack them together to get some really useful (and cool) outputs as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        padding_idx: int = 0,\n",
    "        hidden_size: int = 768,\n",
    "        num_heads: int = 12,\n",
    "        num_layers: int = 12,\n",
    "        dropout: float = 0.1,\n",
    "        activation: nn.Module = nn.GELU(),\n",
    "        feed_forward_size: Optional[int] = None,\n",
    "        mode: str = \"mlm\",\n",
    "        num_classes: Optional[int] = None,\n",
    "    ):\n",
    "        '''\n",
    "        Defines BERT model architecture. Note that the arguments are the same as the default\n",
    "        BERT model in HuggingFace but we'll be training a *much* smaller model for this homework.\n",
    "\n",
    "        Arguments:\n",
    "        vocab_size: The size of the vocabulary (determined by the tokenizer)\n",
    "        padding_idx: The index of the padding token in the vocabulary (defined by the tokenizer)\n",
    "        hidden_size: The size of the hidden layer and embeddings in the transformer encoder\n",
    "        num_heads: The number of attention heads to use in the transformer encoder\n",
    "        num_layers: The number of layers to use in the transformer encoder (each layer is a TransformerEncoderLayer)\n",
    "        dropout: The dropout rate to use in the transformer encoder (what % of times to randomly zero out activations)\n",
    "        activation: The activation function to use in the transformer encoder\n",
    "        feed_forward_size: The size of the hidden layer in the feed forward network in the transformer encoder. If None, defaults to 4 * hidden_size\n",
    "        mode: The mode of the BERT model. Either \"mlm\" for masked language modeling or \"classification\" for sequence classification\n",
    "        num_classes: The number of classes to use in the classification layer.\n",
    "        '''\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        # Now we can put it all together to create the BERT model.     #\n",
    "        #                                                              #\n",
    "        # A BERT model is just a stack of transformer encoder layers   #\n",
    "        #                                                              #\n",
    "        # followed by a pooler layer and a classification layer.       #\n",
    "        # 1. Create a BertPositionalEmbedding layer with the specified #\n",
    "        #    vocab size, hidden size, and padding index.               #\n",
    "        # 2. Create a stack of transformer encoder layers with the     #\n",
    "        #    specified number of layers, hidden size, number of heads, #\n",
    "        #    dropout, activation function, and feed forward size.      #\n",
    "        # 3. Create an MLMHead layer with the specified vocab size and #\n",
    "        #    padding index.                                            #\n",
    "        # 4. Create a Pooler layer with the specified hidden size.     #\n",
    "        # 5. Create a classification layer with the specified number   #\n",
    "        #    of classes.                                               #\n",
    "        #\n",
    "        # HINT: you can use nn.ModuleList to stack layers.  \n",
    "        #\n",
    "        ################################################################\n",
    "        self.mode = mode\n",
    "\n",
    "        self.embedding = BertPositionalEmbedding(vocab_size, hidden_size, padding_idx)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [TransformerEncoderLayer(hidden_size, num_heads, dropout, activation, feed_forward_size)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # stacked_encoders = []\n",
    "        # for i in range(num_layers):\n",
    "\n",
    "        #     stacked_encoders.append(TransformerEncoderLayer(hidden_size, num_heads, dropout, activation, feed_forward_size))\n",
    "\n",
    "        # self.encoder_layers = nn.ModuleList(stacked_encoders)\n",
    "\n",
    "        self.mlm_head = MLMHead(self.embedding.word_embeddings)\n",
    "\n",
    "        self.pooler = Pooler(hidden_size)\n",
    "\n",
    "        if mode == \"classification\" and num_classes is not None:\n",
    "\n",
    "            self.classification = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        self.apply(self.init_layer_weights)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        mask: Optional[torch.Tensor] = None, \n",
    "    ) -> torch.Tensor:\n",
    "        '''\n",
    "        arguments:\n",
    "        x: The input token ids\n",
    "        mask: The attention mask to apply to the input (see the collate function below)\n",
    "        '''\n",
    "        \n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        #\n",
    "        # 1. Get the embeddings for the input token ids\n",
    "        # 2. Calculate the attention weights for each layer (be sure to save the attention weights to return)\n",
    "        # 3a. If the mode is \"mlm\", pass the embeddings through the MLM head and return the output\n",
    "        # 3b. If the mode is \"classification\", pass the embeddings through the classification head\n",
    "        # 4. Return the output (from the relevant head) and the attention weights\n",
    "        #\n",
    "        ################################################################\n",
    "\n",
    "        # print(f\"Input token ids: {x.shape}\")\n",
    "        # print(f\"Mask shape: {mask.shape}\")\n",
    "\n",
    "        embeddings = self.embedding(x)\n",
    "\n",
    "        # print(f\"embeddings shape: {embeddings.shape}\")\n",
    "        \n",
    "        attention_weights = []\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            \n",
    "            embeddings, attn_weights = encoder_layer(embeddings, mask)\n",
    "            attention_weights.append(attn_weights)\n",
    "        \n",
    "        if self.mode == \"mlm\":\n",
    "\n",
    "            output = self.mlm_head(embeddings)\n",
    "            \n",
    "        elif self.mode == \"classification\":\n",
    "            pooled_output = self.pooler(embeddings[:,0])\n",
    "            output = self.classification(pooled_output)\n",
    "\n",
    "            # output = self.classification(embeddings[:, 0])\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "        # pass\n",
    "\n",
    "    def init_layer_weights(self, module):\n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        #\n",
    "        #  Initialize the weights of the model with mean 0 and std 0.02\n",
    "        # \n",
    "        ################################################################\n",
    "        if isinstance(module, (nn.Linear)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 30522])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We can verify that the model is working by running a quick test.\n",
    "'''\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = tokenizer.encode(sentence)\n",
    "# to tensor\n",
    "token_ids = torch.tensor(tokens.ids)\n",
    "\n",
    "bert = BERT(vocab_size=tokenizer.get_vocab_size(), \n",
    "            hidden_size=128, \n",
    "            num_heads=4, \n",
    "            num_layers=2,\n",
    "        )\n",
    "\n",
    "attention_mask = (token_ids != 0).float()\n",
    "\n",
    "output, attn = bert(token_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Training\n",
    "\n",
    "After all that implementation, it's time to pre-train our BERT model. We'll focus specifically on MLM training and then in a separately notebook you'll used the pre-trained BERT for Parts 5 and 6. \n",
    "\n",
    "To pre-train, we'll need to accomplish three pieces:\n",
    "1. First, we'll need to create a `Dataset` class that says how to load and process our text data for MLM training. \n",
    "2. Second, we'll need to create a `collate` function that tells the `DataLoader` how to combine multiple training examples from our dataset into a batch. The `collate` function is critical because not all of our input texts have the same size (different sequence lengths) which will create a wrinkle for giving a model a single `Tensor`\n",
    "3. Third, we'll write the core training loop.\n",
    "\n",
    "Each of the pieces below goes over more details. As you progress as a practitioner, you'll frequently need to write `Dataset` and `collate` functions for more bespoke kinds of training tasks. This part of the assignment is intended to show you how to do some simple implementations that you can re-use later. You'll likely reuse some/all of this code in Parts 5 and 6 when setting up these functions for classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.1: Create A Dataloader for Masked Language Modeling\n",
    "\n",
    "A `Dataset` is kind of like a glorified `list` object in python. The Dataset class helps us to load and process the data, which providing functionality that lets us access the data\n",
    "\n",
    "The main function of the Dataset class is to get the number of samples and to get a sample from the dataset. The core functionality you'll need to implement in this part of the assignment is the following:\n",
    "- Load the data and tokenize it using the tokenizer.\n",
    "- The __len__ method to return the number of samples in the dataset.\n",
    "- The __getitem__ method to return a sample from the dataset.\n",
    "\n",
    "As a sidenote, the `Dataset` class provides a very important abstraction for training and more sophisticated `Dataset` implementations will do usfeul things like keep only some of the dataset in memory and proactively fetch data from desk to keep the overall memory footprint low. Others may do pre-processing on the fly to avoid having large `Tensor` objects in memory (e.g., loading images and preparing them for image-based learning). Just think of how large some datasets might get---as a practitioner, these implementations are critical for efficient training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data: list[str], max_seq_length=128, mlm_probability=0.15):\n",
    "\n",
    "        ##################################################################################################################\n",
    "        # TODO: YOUR CODE HERE \n",
    "        # \n",
    "        # 1. Store the arguments as fields\n",
    "        #                                                                                                                #                                    #\n",
    "        ##################################################################################################################\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.mlm_probability = mlm_probability\n",
    "\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def tokenize(self):\n",
    "        '''\n",
    "        Tokenizes the text in self.data, performing any preprocessing and storing the tokenized data in a new list.\n",
    "        '''\n",
    "\n",
    "        ##################################################################################################################\n",
    "        # TODO: YOUR CODE HERE \n",
    "        #                                                                                                                #                                    #\n",
    "        # 1. Tokenize the data using the tokenizer.                                                                      #\n",
    "        # 2. Truncate the sequence to the maximum sequence length.                                                       #\n",
    "        # 3. Add the tokenized data to a list.                                                                           #\n",
    "        #\n",
    "        # NOTE: To save memory, you can delete self.data after tokenizing since you'll have the copy of \n",
    "        #       the tokenized data (as ids) and won't need the raw text later.\n",
    "        #\n",
    "        ##################################################################################################################\n",
    "        tokenized_data = []\n",
    "        for text in self.data:\n",
    "            \n",
    "            tokens = self.tokenizer.encode(text).ids\n",
    "\n",
    "            tokens = tokens[:self.max_seq_length] # might need to do -1 and add a sep?\n",
    "            tokenized_data.append(tokens)\n",
    "        self.data = tokenized_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns the list of the token ids of an instance in the dataset and a list of the labels for MLM (one label per token).\n",
    "        '''\n",
    "\n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        #\n",
    "        # 1. Get the tokenized data at the specified index\n",
    "        # 2. Create the mask (i.e., which words the model has to predict). Special tokens are never masked.\n",
    "        #    Non-masked tokens should be set to -100, which will be ignored in the loss function. \n",
    "        #    The masked tokens should be set to the original token ids. Use the specified masking probability.        \n",
    "        # \n",
    "        # Hint: Use the tokenizer's functions to get IDs as needed\n",
    "        ################################################################\n",
    "\n",
    "        token_ids = self.data[idx] # for text if tokenize does for token above\n",
    "\n",
    "        # print(len(token_ids))\n",
    "\n",
    "        input_ids = deepcopy(token_ids)\n",
    "        labels = deepcopy(token_ids)\n",
    "\n",
    "        for i, token_id in enumerate(token_ids):\n",
    "\n",
    "            if token_id in [101, 102, 0]:\n",
    "                labels[i] = -100\n",
    "                \n",
    "            else:\n",
    "\n",
    "                if random.random() < self.mlm_probability:\n",
    "\n",
    "                    input_ids[i] = self.tokenizer.token_to_id(\"[MASK]\")\n",
    "        \n",
    "\n",
    "                else:\n",
    "                    labels[i] = -100\n",
    "    \n",
    "        return input_ids, labels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [101, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 103, 103, 102]\n",
      "labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1046, 1047, -100]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Verify that the dataset is working by running a quick test.\n",
    "'''\n",
    "\n",
    "# create a fake dataset using 26 aphabets, 1000 sentences, 10-20 words per sentence randomly\n",
    "data = [' '.join([chr(97 + i) for i in range(random.randint(10, 20))]) for _ in range(1000)]\n",
    "\n",
    "dataset = MLMDataset(tokenizer, data)\n",
    "dataset.tokenize()\n",
    "\n",
    "# get the first item\n",
    "input_ids, labels = dataset[0]\n",
    "print('input_ids:', input_ids)\n",
    "print('labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [101, 1996, 4248, 103, 4419, 14523, 2058, 1996, 13971, 3899, 1012, 102]\n",
      "labels: [-100, -100, -100, 2829, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "## tet with one sentence\n",
    "data = [\"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",]\n",
    "dataset = MLMDataset(tokenizer, data)\n",
    "dataset.tokenize()\n",
    "\n",
    "input_ids, labels = dataset[0]\n",
    "print('input_ids:', input_ids)\n",
    "print('labels:', labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 2428, 2092, 103, 103, 1012, 1045, 2031, 103, 2009, 2320, 1998, 2097, 3191, 2009, 2153, 2574, 1012, 102], [-100, -100, -100, 2517, 2338, -100, -100, -100, 3191, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])\n"
     ]
    }
   ],
   "source": [
    "# Let's generate with our real data!\n",
    "review_data_path = 'test.txt'\n",
    "# review_data_path = './reviews-word2vec.med.txt' # <- for sanity checking / debugging\n",
    "# review_data_path = './reviews-word2vec.large.txt' # <- for CPU pre-training and validating\n",
    "#review_data_path = './reviews-word2vec.larger.txt.gz' <- for GPU pre-training and validating (Part 3.5)\n",
    "\n",
    "\n",
    "# NOTE: when you eventually deploy this code to Great Lakes, you'll need to use the larger dataset \n",
    "# (see the PDF for notes/details)\n",
    "\n",
    "ofunc = gzip.open if review_data_path.endswith('gz') else open\n",
    "with ofunc(review_data_path, 'rt') as f:\n",
    "    reviews = f.readlines()\n",
    "    reviews = [review.strip() for review in reviews]\n",
    "\n",
    "dataset = MLMDataset(tokenizer, reviews)\n",
    "dataset.tokenize()\n",
    "\n",
    "# get the first item\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.2: Create A `collate` function to prepare a batch for training\n",
    "\n",
    "Once the dataset is ready, we can use the `DataLoader` class  to load the data from the `Dataset` class, much like you did for Homework 2.  Remember that the Dataset class has the __getitem__ method  that returns the input_ids and the labels.\n",
    "    \n",
    "Like Homework 2, we'll want to train using a _batch_ of items. This is good for two reasons. First, batching helps us learn from multiple examples at the same time, so the gradient is a bit smoother. Batching provides a good trade-off between SGD (one item at a time) and full GD (all items at once). Second, and perhaps more importantly, batching allows us to maximize the throughput of our computing resources. Depending on the hardware, some matrix operations are the same speed for different sized matrices, so if we can get more examples used to trained per step, this reduces the overall number of steps. You may have seen this in Homework 2 where increasing the batch size dropped the training time, up to some point. \n",
    "\n",
    "By default, the `DataLoader` will randomly sample $b$ items from `Dataset` where $b$ is the batch size and turn those into a single `Tensor` to pass as input to the model. To create the batch itself, the `collate` function will tell the `DataLoader` how to turn multiple items into a single `Tensor`.\n",
    "\n",
    "However, we have a major wrinkle here. When we train BERT for MLM over sequences, not all the sequences have the same length. A batch itself is represented as a `Tensor`. When all the instances in the batch have the same length, we can concatenate/stack them. For example, if we had 10 instances of sequences of length 5, we can create a tensor that is size (10, 5). However, if some of those sequences have different lengths, we no longer can create a Tensor with a single length dimension! What to do?\n",
    "\n",
    "To solve this, we'll need to write the `collate` function so that it makes all the sequences have the same length. Typically this is done by adding a special `[PAD]` token to the sequences so that they all have the same number of tokens. However, this extra token is meaningless!  If we don't recognize this, then our model will learn to predict `[PAD]` tokens (yikes!). Therefore, we also need to create an _attention mask_ that tells us which tokens to ignore in the input because they are padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List[Tuple[List[int], List[int]]]):\n",
    "    '''\n",
    "    A function that takes a list of instances in the dataset and collates them into a batch.\n",
    "    '''\n",
    "\n",
    "    ################################################################\n",
    "    #                    TODO: YOUR CODE HERE                      #\n",
    "    # 1. Separate the input_ids and the labels into two lists      #\n",
    "    # 2. Pad both lists using the tokenizer's padding value so \n",
    "    #    that all lists are the same length.\n",
    "    # 3. Create a boolean attention mask for the input_ids which specifies\n",
    "    #    which tokens are non-padded elements.\n",
    "    # 4. Return the padded input_ids, the attention mask, and the padded labels.\n",
    "    #\n",
    "    # NOTE: Look at nn.utils.rnn.pad_sequence\n",
    "    #\n",
    "    ################################################################\n",
    "    \n",
    "    # Separate the input_ids and the labels into two lists\n",
    "    input_ids, labels = zip(*batch)\n",
    "\n",
    "    # Pad both lists using the tokenizer's padding value so that all lists are the same length\n",
    "    padded_input_ids = nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=tokenizer.token_to_id(\"[PAD]\"))\n",
    "    padded_labels = nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in labels], batch_first=True, padding_value=-100) \n",
    "\n",
    "    # Create a boolean attention mask for the input_ids which specifies which tokens are non-padded elements\n",
    "    attention_mask = (padded_input_ids != tokenizer.token_to_id(\"[PAD]\")).long()\n",
    "\n",
    "    return padded_input_ids, attention_mask, padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 28])\n",
      "torch.Size([4, 28])\n",
      "torch.Size([4, 28])\n",
      "tensor([  101,  1045,  5959,  1037,  2293,  9546,  1998,  2122,  2808,   103,\n",
      "         2009,   103,  2059,  2070,   999,   103,   103,  1996,  3494,  1998,\n",
      "        14405,  3524,  2005,  1996,  2279,  2338,  1012,   102])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, 2031, -100, 1998,\n",
      "        -100, -100, -100, 1045, 5959, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n"
     ]
    }
   ],
   "source": [
    "# test the collate function\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# All of the items in the batch should have the same shape!\n",
    "for input_ids, attention_mask, labels in dataloader:\n",
    "    print(input_ids.shape)\n",
    "    print(attention_mask.shape)\n",
    "    print(labels.shape)\n",
    "    break\n",
    "print(input_ids[1])\n",
    "print(attention_mask[1])\n",
    "print(labels[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.3: Train BERT for MLM!\n",
    "\n",
    "Once we have our `Dataset` and `collate` function, it's time to train the pre-model the BERT model for MLM. Working on your laptop to start, try training for 1-2 epochs on the `med` dataset. There's no guarantee the model will learn anything useful, but we can do some experiments later in Part 4 to take a look. \n",
    "\n",
    "The core steps for training are the following:\n",
    "1. Define your model\n",
    "2. Set up `wandb` for tracking its progress (this will be useful for monitoring when running on Great Lakes)\n",
    "3. Define the optimizers, learning rate, etc.\n",
    "4. Define the core training loop\n",
    "\n",
    "The code won't look too dissimilar from past pytorch training loops in Homeworks 1 and 2.\n",
    "\n",
    "Training times can vary, but on an M1 Mac and the default hyperparameters, one epoch takes ~50min on the large dataset with \"mps\" and 6 hours on \"cpu\". The model and training seem to fit at 12GB of memory. \n",
    "\n",
    "For training, we recommend trying to run either the medium or large for a few epochs. If your CPU is slower, try medium just for one epoch to get a sense of what it's learned in Part 4.  For getting a sense of whether the model is working, we strongly recommend testing your CPU-trained model with the masked language modeling task in Task 4.4 to see whether it can correctly fill in common words. The most similar words aren't always a good indicator that it's working.\n",
    "\n",
    "Don't worry if you can't train too long on your own machine. Once you've gotten the model working (e.g., some preliminary analysis in Part 4 looks \"okay\" -- doesn't have to be great), it's time to convert this to a script and run it on Great Lakes in Part 3.5 (more details there). You'll use the advanced GPUs on the cluster to go through 5 epochs (or more) during training, which will give you a good enough model that you can use it for the rest of the homework in Parts 4 and 5.\n",
    "\n",
    "\n",
    "#### VERY OPTIONAL PARTS:\n",
    "\n",
    "If you are feeling _really_ adventurous, you can try to speed up your model by trying a few of the much fancier things in pytorch. In practice, you should not need these for the homework. However, they can be fun to explore even with a CPU, though they make the biggest impact if you have access to a GPU too\n",
    "\n",
    "- Try using [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) to optimize the `nn.Module` code (this tries to pre-compile the computation graph)\n",
    "- Rather than train with 32-bit floating point, use [amp](https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/) to do mixed-precision training (i.e., fewer bits and faster). If you want to test this on Great Lakes, the GPUs there support \"fp16\" which will greatly speed up training. At the moment, `amp` isn't supported on \"mps\" devices, though it might show up [soon](https://github.com/pytorch/pytorch/issues/88415).\n",
    "- Implement an alternative training loop using [`accelerate`](https://github.com/huggingface/accelerate) and try using mixed precision (fp8, fp16, bf16) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'mps' device\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3s1ast6g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47f335c6f574dfb8aa26b325d7702f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Loss</td><td>3.4702</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">BERT_for_MLM_test2</strong> at: <a href='https://wandb.ai/mcinnisd/Homework3/runs/3s1ast6g' target=\"_blank\">https://wandb.ai/mcinnisd/Homework3/runs/3s1ast6g</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240328_235553-3s1ast6g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3s1ast6g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b53505b0324f61ba357504051721ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167261566717773, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/davidmcinnis/Dropbox (University of Michigan)/UM/W24/EECS595/hw3/wandb/run-20240328_235735-ci4l8c36</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mcinnisd/Homework3/runs/ci4l8c36' target=\"_blank\">BERT_for_MLM_test2</a></strong> to <a href='https://wandb.ai/mcinnisd/Homework3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mcinnisd/Homework3' target=\"_blank\">https://wandb.ai/mcinnisd/Homework3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mcinnisd/Homework3/runs/ci4l8c36' target=\"_blank\">https://wandb.ai/mcinnisd/Homework3/runs/ci4l8c36</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7974f57b8d894f15a1b5a55ec7e688e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59374410c7e34bc881940628263cf604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250, Avg. Loss: 117.85022735595703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f8bb456720408c80e04dae84286a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/250, Avg. Loss: 116.66798400878906\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfa6d6e16c646c3bd61e4cf6ceb6640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/250, Avg. Loss: 110.7553939819336\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35a0c162a4e4c78865664b49dfef115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/250, Avg. Loss: 128.71981811523438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c19502f11b478992ef032c8392f69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/250, Avg. Loss: 115.81653594970703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3713beaf6b4123bdcd82ad2325c7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/250, Avg. Loss: 107.08267974853516\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f5818f5802412f8f993b38361d3482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/250, Avg. Loss: 100.83059692382812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fa2f600d4d4ab3a974318b6bd5b926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/250, Avg. Loss: 102.76039123535156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b00f1d0d0e94b739f5b9d0f82339e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/250, Avg. Loss: 106.93006896972656\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824b4c78b4734280a6ff31ea820d995d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/250, Avg. Loss: 108.49053192138672\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f282408e34423ea3190c68bfd80dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/250, Avg. Loss: 96.24567413330078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc848af0a0ab4949b0d2fc9fa37efe65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/250, Avg. Loss: 90.81875610351562\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a952573b64254b1898d947ed3d5dd227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/250, Avg. Loss: 83.41717529296875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770ce9b2d7fa49b2945f82076e1a4fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/250, Avg. Loss: 89.96137237548828\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de89633ca93d48868e4f89b4524e2f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/250, Avg. Loss: 95.5261001586914\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3856b6fa784dcd97d7b9a71065c96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/250, Avg. Loss: 93.5631332397461\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a2c7511e8c49d68ef3091ed278b210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/250, Avg. Loss: 88.046142578125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd8257db83f49c9a55594f702470912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/250, Avg. Loss: 88.07342529296875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8db80aaa9847f5be014a66c74cf938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/250, Avg. Loss: 83.75992584228516\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863f050562f54932bee2981f6e6caf43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/250, Avg. Loss: 77.5954360961914\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ce33a3c05747b19c51435c47e34a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/250, Avg. Loss: 73.78656005859375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d246cc868465459fb0d9efc0b5ab4bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/250, Avg. Loss: 72.31837463378906\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9857c52cea2a478a8f5c08e18aa4913f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/250, Avg. Loss: 61.39542770385742\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39df16ad0ae54e3b9068d7d7d1438bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/250, Avg. Loss: 70.3487777709961\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9921c9e52c9b46a9b569113887e71370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/250, Avg. Loss: 58.23622131347656\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36be59d065be4057b6f81ebdeeda4641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/250, Avg. Loss: 64.46070861816406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d52f161c6324e18bb59c274956e20d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/250, Avg. Loss: 64.09156799316406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e07088200594927a248a348c1f4dcb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/250, Avg. Loss: 55.7281494140625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8027fc43f4d240319003581b2e4965f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/250, Avg. Loss: 65.79239654541016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23232b68789a4dc7903bca64503bf3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/250, Avg. Loss: 55.026283264160156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0fd8412c71429cb291ce0f4ec3bf64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/250, Avg. Loss: 51.33205795288086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a292c13f5b0e4f8292b0cc4a2e7ad2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/250, Avg. Loss: 51.19139099121094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6bd0be35ba4b938571d1e4ee482c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/250, Avg. Loss: 47.57199478149414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a963ef9bd64ca98b38159104b9867a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/250, Avg. Loss: 46.314151763916016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61b9da0d220468898dbea95f51c42e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/250, Avg. Loss: 51.372066497802734\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d9000bfdfe483fa5a35572a8f208c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/250, Avg. Loss: 45.1083869934082\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1b928acc5b43ca955369b30ed98479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/250, Avg. Loss: 40.051841735839844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b8b50a258540229ecbe129b5cc0935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/250, Avg. Loss: 45.3896369934082\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3ef7fa74fe41e290696bd65626105b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/250, Avg. Loss: 36.861637115478516\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d983a4b43684abdbd3fee7bb0045816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/250, Avg. Loss: 40.937217712402344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8eeb09b18f04901b1f67e17c0e35c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/250, Avg. Loss: 47.67729187011719\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe84c94ffa64b2aae42ccc385148ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/250, Avg. Loss: 47.05575180053711\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ed4c5dd21e4cc0bb3465b74197a625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/250, Avg. Loss: 44.25510025024414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b15a7484624b828ba1ac5d90d588a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/250, Avg. Loss: 41.245994567871094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ac15cef0a841dfa77fc9c1dbe5144a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/250, Avg. Loss: 40.7358512878418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e1bf358729416089943db39f35a111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/250, Avg. Loss: 48.381526947021484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70e0b5310d44b34a13163f189832bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/250, Avg. Loss: 38.70146179199219\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94315992553b4a29a2406b6ea9b4b5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/250, Avg. Loss: 38.94218444824219\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061a7fc043e340a086d922d721eea21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/250, Avg. Loss: 40.6181640625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79de7d15adbc4808bac8ff9ed26ec8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/250, Avg. Loss: 37.81570816040039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563e86788bca495bac5533f9664c62da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/250, Avg. Loss: 41.86882019042969\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77235ef7bc5473f87bf685d5e53d542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/250, Avg. Loss: 39.70458984375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4715f8f4182140a79a83d8ab36878783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/250, Avg. Loss: 39.41477966308594\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fdcf33640f4d28b4441198e0f0d1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/250, Avg. Loss: 48.66706085205078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a424c4d601c0447e844069e1d55dbcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/250, Avg. Loss: 30.073619842529297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa719b307064a41a4b363259565d64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/250, Avg. Loss: 37.044761657714844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6101963c674c08ba56f5c62dccccfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/250, Avg. Loss: 35.20732498168945\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fe81649231456784e640c3a023f594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/250, Avg. Loss: 41.23366928100586\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6560ee97e01048e4831c00edf640c018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/250, Avg. Loss: 39.69068145751953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa04d67794b644a799806c0b906e5643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/250, Avg. Loss: 32.93550491333008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b43183e1f240bd99a70f8fe7c956a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/250, Avg. Loss: 32.20873260498047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fd9fe6134b42e592f81eac7c429253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/250, Avg. Loss: 35.362972259521484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f4aff43b4b4caf9deadd99481efce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/250, Avg. Loss: 34.30188751220703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78f4a48fe534912ba4a9b18734169ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/250, Avg. Loss: 29.577470779418945\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f591f90af97c4fcc94ebd6f1c1a2a68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/250, Avg. Loss: 36.18525695800781\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aee4b56c73149d3bcc115b128879d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/250, Avg. Loss: 42.9203987121582\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b05ce3f0aef46c9bd6372ffb36cefdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/250, Avg. Loss: 36.45378875732422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fabc0a7f16248309b7644e05e02dcff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/250, Avg. Loss: 35.183204650878906\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73441e51ac22444ebe9494d363ad099e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/250, Avg. Loss: 33.501861572265625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f33374339c406dafa2f839b5b15242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/250, Avg. Loss: 39.488014221191406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9433d6bfb4242559cb00bbd8dcab830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/250, Avg. Loss: 36.49122619628906\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280ac934759b4c1e9d0ddbd705704b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/250, Avg. Loss: 25.39194107055664\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806d2ee27cac44e79d7d3ea4ee4815ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/250, Avg. Loss: 30.64529037475586\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082ca365a261492e97fa857306210c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/250, Avg. Loss: 31.331188201904297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47ae03f88174708a9e443dfb68a8213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/250, Avg. Loss: 29.41842269897461\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdf88a06e834968b4c474ede4123096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/250, Avg. Loss: 26.70718765258789\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbff304a73a84274a99ba701b095036c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/250, Avg. Loss: 30.60302734375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc12e78f0bc4d22803e71931beca92c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250, Avg. Loss: 29.073326110839844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac26cf9460164165a58327bf500b7202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/250, Avg. Loss: 31.877656936645508\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0887d06ed54db09fb9ca2ce566664d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/250, Avg. Loss: 26.720746994018555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cfc288e9e24b65a3d47f24b6c85df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/250, Avg. Loss: 23.18079376220703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729f74859e804b129a3be6e17243f850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/250, Avg. Loss: 29.653554916381836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9e315cb24e4a6ea46a172e85ea4e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/250, Avg. Loss: 23.28282356262207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7b4bfcbb534aa5ac01f1c96c8bb0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/250, Avg. Loss: 26.104082107543945\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b29848ec78a478ab596a12669ba54b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/250, Avg. Loss: 32.497581481933594\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af41fe7f5135438d9257dc52242f827e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/250, Avg. Loss: 31.742036819458008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f564046fea5240e0a85c6b858432ffc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/250, Avg. Loss: 25.901607513427734\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9beacb28427e46bdbd12f85fb5ecf6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/250, Avg. Loss: 27.611604690551758\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ef0b8ccc1746daa72e9beaf4a0410b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/250, Avg. Loss: 24.24799156188965\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4245aba267b4671aa8dc03ea3fe011c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/250, Avg. Loss: 18.17544174194336\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7199b51351742e88397308e0335883b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/250, Avg. Loss: 28.608549118041992\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d314ac7e24764c8fa4468c5bb675318b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/250, Avg. Loss: 28.772998809814453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f750abb5c6a43c7921fd0538b645f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/250, Avg. Loss: 20.155332565307617\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6536cfe48314d8fa9107394005f02b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/250, Avg. Loss: 34.89704513549805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff128cb31eb495093f67e6df26cd07b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/250, Avg. Loss: 25.989517211914062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f873e6be92a6431e9c43d567b8ceeac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/250, Avg. Loss: 24.807897567749023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70e4495b89143b0a7058c9633dd9949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/250, Avg. Loss: 28.247474670410156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4ed474a763453e842cc405f255f22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/250, Avg. Loss: 16.730571746826172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75264fcabf954206ad04b2570cd63141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/250, Avg. Loss: 22.42760467529297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c11f222e414084882f9e4b4efc4e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/250, Avg. Loss: 25.064632415771484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edb7c66e8384f5d9ae32229755266dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/250, Avg. Loss: 30.967164993286133\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36a3f2e6321451fa1c4a70953eb1eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/250, Avg. Loss: 22.790040969848633\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52f061e77644630ab04bdf81524dd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/250, Avg. Loss: 30.113178253173828\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ece432ff1546e5867d76d282cf4dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/250, Avg. Loss: 20.061494827270508\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa0b19a22fb48b6879392dba56ecbae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/250, Avg. Loss: 16.902551651000977\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2dd7da126c4b528820f1da9084eb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/250, Avg. Loss: 20.58463478088379\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14a9964b83f4b7198af11351f9ef976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/250, Avg. Loss: 19.611278533935547\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72aac1ac22a247f3a1415a2e95fbee26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/250, Avg. Loss: 20.19377899169922\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6063b1252f409ebd8eee2ee0cfa95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/250, Avg. Loss: 18.43961524963379\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e3a55c0f88437782d7c456930fe762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/250, Avg. Loss: 23.81696128845215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651c4373555844deb694c251b0c3e9e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/250, Avg. Loss: 21.179121017456055\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e359beeeb9344ccd806b751cd84816ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/250, Avg. Loss: 21.824234008789062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8917ce81125c4331916414f9f7620183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/250, Avg. Loss: 25.72568702697754\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44870e7581c4b2b965036f82f6dd8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/250, Avg. Loss: 19.12978172302246\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df70ea1e3fc4b4bbf62aab989d54918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/250, Avg. Loss: 17.725679397583008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27dc1f21de694e4bab0ab3a07dde8917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/250, Avg. Loss: 22.75425910949707\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b3cde54d9044d09396044ea2b40076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/250, Avg. Loss: 21.979169845581055\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c03e3b213d4e86845d4f0658516f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/250, Avg. Loss: 19.14869499206543\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba24f58ab00473ead462974ed938768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/250, Avg. Loss: 18.42888069152832\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0076fe4761f248b7bb74eb2babd9a9dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/250, Avg. Loss: 22.296375274658203\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed6895909be4a8485ae8c3c23138c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/250, Avg. Loss: 17.442113876342773\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc68b60bbd147079b351171a056f0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/250, Avg. Loss: 18.531600952148438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4385ae548f4eb390dfd4dea9ecbef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/250, Avg. Loss: 12.343816757202148\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9231ddf134424c36a888ac4fcc1950cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/250, Avg. Loss: 18.69261360168457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17eb3046bf2547ef955947814d973a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/250, Avg. Loss: 19.918079376220703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58aadfc1fb574de2b5ac97fe8c43e3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/250, Avg. Loss: 20.732105255126953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77eca86bd1c4162a91d5f9c592e4dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/250, Avg. Loss: 29.20296287536621\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6fe263cb4d4e9f9ca69d13e2965ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/250, Avg. Loss: 18.231216430664062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deccde1dc469418891a773b06bf710f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/250, Avg. Loss: 18.18680763244629\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89ceb5c14404389b76bac0d38a3c808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/250, Avg. Loss: 23.74791717529297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d51efe2a3f43d796e0b678de53d1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/250, Avg. Loss: 17.443334579467773\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebc9c57bd314ae09224d5141d589e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/250, Avg. Loss: 20.237064361572266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d554cad8649a4baab1f0b42dca728429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/250, Avg. Loss: 14.348870277404785\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c882ffb4f643b68fde40f06dd3f1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/250, Avg. Loss: 22.607656478881836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ce6fa0512243b18851e5daf4a87532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/250, Avg. Loss: 14.214086532592773\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f133bb965c41d8bf5d80376669b03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/250, Avg. Loss: 16.88498878479004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865a5777c7c14ad294e9d4e6015d4fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/250, Avg. Loss: 17.364574432373047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b5b14210054bc7adfc421ed66982a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/250, Avg. Loss: 13.579416275024414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487608f119084684b961f88f0f06c6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/250, Avg. Loss: 13.20116901397705\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8787f4d8eb8d433ea1a49778ad975407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/250, Avg. Loss: 14.1768159866333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab526d0f498b43f9803104ddd57c22e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/250, Avg. Loss: 16.371427536010742\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028f9a66cffc4eb3b73eac6dfb9df08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/250, Avg. Loss: 18.06730079650879\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d1e979b9114abb8e7bf05b45de44a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/250, Avg. Loss: 14.77754020690918\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876f20cbb36849e6810dda1b6bfddba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/250, Avg. Loss: 12.789190292358398\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1441969106743e192b42635e166fead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/250, Avg. Loss: 14.428193092346191\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e9b3f4b506468db613276ea8cf4ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/250, Avg. Loss: 11.871408462524414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29cb8421bb1f4f1b83ff07e1d53910a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/250, Avg. Loss: 13.431897163391113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98579466d3c843da8f67457d31e6b3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/250, Avg. Loss: 21.438297271728516\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f312a5f57eee40668f481585dbf24f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/250, Avg. Loss: 9.00503158569336\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd03a6538fac42dfa62082b4d8c9fcd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/250, Avg. Loss: 15.990853309631348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acafacac099491a8845d4a0535fc60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/250, Avg. Loss: 9.45924186706543\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dc9639e30c4873bfa9d822ce5ad1cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/250, Avg. Loss: 14.939860343933105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca98728619f431a93a72fbf35e927a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/250, Avg. Loss: 7.805417537689209\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d084688ee148378282245abced19e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/250, Avg. Loss: 11.344502449035645\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472040bcbc0344319cd10a9ed17d0393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250, Avg. Loss: 13.449569702148438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072b4f354fcd4818898df55102996feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/250, Avg. Loss: 11.373787879943848\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa659e14965a4b5c817ec25866aa0a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/250, Avg. Loss: 13.289761543273926\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3524ce7e754a45b8bf5d0301df7a0993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/250, Avg. Loss: 12.539770126342773\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb28420bfb6459494efeb531d304d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/250, Avg. Loss: 6.120179653167725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd06c057ef3482091d89e132f397953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/250, Avg. Loss: 6.8759894371032715\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99200186e8484067b482e3e55b6c1847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/250, Avg. Loss: 13.035135269165039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0387a31cb383410cb96787722ed379eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/250, Avg. Loss: 10.56723690032959\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011790115e394316b886242958338f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/250, Avg. Loss: 14.216425895690918\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a5fe68df794069b72476f128646962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/250, Avg. Loss: 18.15632438659668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471f0751c58e47d099af274b2c2eba35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/250, Avg. Loss: 11.481173515319824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8faa8796e1174cf5afff0166af9b2455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/250, Avg. Loss: 8.910208702087402\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6af3e1cf8b4037b73a414ea4a09ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/250, Avg. Loss: 13.977930068969727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fed169ca06543ae81b772b056ab2781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/250, Avg. Loss: 9.085671424865723\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c134ab7985c42538fbd5a4705a81f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/250, Avg. Loss: 13.695409774780273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136a137fcb194dc195fd516164137bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/250, Avg. Loss: 16.324962615966797\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a463087749f432cb3f5106a831f9ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/250, Avg. Loss: 8.760466575622559\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2076939e83414e2a9e56fd8b5fc9b9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/250, Avg. Loss: 9.826635360717773\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a16aa114254aeaa5c3b9f56a65a864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/250, Avg. Loss: 11.260794639587402\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd18c70801b84880ac4a1317f0d321b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/250, Avg. Loss: 8.105331420898438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a5de94c4b9476a84b506243946a2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/250, Avg. Loss: 9.561530113220215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85f6dcee1c04ba3b371b8edfab11064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/250, Avg. Loss: 6.431335926055908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41ff48b71c54f1aaa4a03529b51492b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/250, Avg. Loss: 9.725481986999512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b726990ae1ae4a6b8a3d7f52852afc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/250, Avg. Loss: 6.42694091796875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4b0b71f727498f93c38d67efbf54e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/250, Avg. Loss: 6.556130886077881\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b269db97af46a4b33983726a369829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/250, Avg. Loss: 7.19525146484375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a880be2d2b4743af6b74dcba3305fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/250, Avg. Loss: 12.468096733093262\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f091b31f56ce49a2b138d473191cd8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/250, Avg. Loss: 8.076319694519043\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af72cff7d1174e43a70f4a4a8e697a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/250, Avg. Loss: 9.29233169555664\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41bc071440ed4fbcb3e93ee4345f37b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/250, Avg. Loss: 7.4763617515563965\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106f590bfdbd417486b0249a7bdc43f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/250, Avg. Loss: 5.2193403244018555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a93773b540346bba2cf64328ef53f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/250, Avg. Loss: 10.344356536865234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33376dd6771d4ece817063b9f7e2e8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187/250, Avg. Loss: 5.7597222328186035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9a24262ed0479ab9f307ab28d718ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/250, Avg. Loss: 6.637601375579834\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c65104128e648c99dad06b6deecd9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/250, Avg. Loss: 8.13858699798584\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed1341e61744e22ae254ecb36bf268b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/250, Avg. Loss: 9.000988960266113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0443c1eab34612b378f6b92b24c033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/250, Avg. Loss: 8.094148635864258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23ce8c8d53f4eb896fb97a72a2c3836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/250, Avg. Loss: 6.085808277130127\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b5aaa6da8b41498a73677cc0c815b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/250, Avg. Loss: 6.652058124542236\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6880acf8de754fa2a31963171306632c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194/250, Avg. Loss: 5.247047424316406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af0e623037447a8903f4ef9cb334f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195/250, Avg. Loss: 11.427306175231934\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c440b6ab1f3e4ab48aee49234f0769c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/250, Avg. Loss: 8.550183296203613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1782f2e73ca41d5a438a9a42dfdb726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/250, Avg. Loss: 6.228885650634766\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d98934cb174b79891f4c57434023ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/250, Avg. Loss: 7.827038288116455\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfd4d3cd91444059ea3fca3e9b72866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/250, Avg. Loss: 7.735525131225586\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498eb429f3f149158c792bcf5f7c550d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/250, Avg. Loss: 2.7996349334716797\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74510f8f46b4a889a4eeaf2f12169a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/250, Avg. Loss: 2.6870155334472656\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2851b5919ad4ed68a5dec45eb56d0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202/250, Avg. Loss: 2.3930954933166504\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbff1351e6594506b12834162600a0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/250, Avg. Loss: 5.6135663986206055\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77aebd382f9b4caf99ab07daf228ea5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204/250, Avg. Loss: 4.205855369567871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdd53a4bde14f61aea70437d36a0dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205/250, Avg. Loss: 13.105905532836914\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d348f65b7d6f45419c9793b93ad872b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 206/250, Avg. Loss: 2.937091588973999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2ff419b0bb4fe3b89037cc43d6b1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207/250, Avg. Loss: 2.9504802227020264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf938158c35b484e86d948b0998450ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208/250, Avg. Loss: 5.543986797332764\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347c664954bd43bbb0282a6a9fc7567f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209/250, Avg. Loss: 1.9138340950012207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d3c799c650439aa12af314a5c4ba7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210/250, Avg. Loss: 4.626967906951904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203f7816e20e469ba12d496974884264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211/250, Avg. Loss: 4.277947425842285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1481af26de4270887e514ebae85e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212/250, Avg. Loss: 2.2267515659332275\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86f7c555fc346628847b549c5d5070f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/250, Avg. Loss: 3.697110891342163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583e522ca5de4e73a511c129bca45e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214/250, Avg. Loss: 2.90716814994812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10bd31eaa564afbb8722d3885206193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215/250, Avg. Loss: 2.8396661281585693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad8f47298694a74bf26cd4a7f261b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216/250, Avg. Loss: 8.278322219848633\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c98f091a0b4fa690566f546d75aecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/250, Avg. Loss: 4.657436847686768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d7ecdad98f4740933291560f066a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 218/250, Avg. Loss: 5.109104633331299\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a7f0ee7942443a8db0496b488b4de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 219/250, Avg. Loss: 1.947661280632019\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9280a5fdf2ad467a95f3cb4f5579a7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/250, Avg. Loss: 1.0383318662643433\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01d686bc4964330934ed3749ac4e7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/250, Avg. Loss: 2.996260404586792\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573b27fcefdd4347a7469c16a9994968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/250, Avg. Loss: 2.716637134552002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e982a89e9e2a48e094b8bcea519c8753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/250, Avg. Loss: 2.287914514541626\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e052a10e8d84c39b24abc74423b8628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/250, Avg. Loss: 2.5543689727783203\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b447efcd5264e9a8fb27250057570fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/250, Avg. Loss: 3.5000672340393066\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f307ca2a074918b0e3284198180560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/250, Avg. Loss: 4.03662633895874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c15e9e492de40548a29db9076071b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/250, Avg. Loss: 0.38903701305389404\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7944011a21f4d009fced41683cd0f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/250, Avg. Loss: 0.8244940638542175\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff7b1d67ee84b209f6eb934bac6ceae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/250, Avg. Loss: 3.3530073165893555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079a1d50ba1d4a59899b462544315236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/250, Avg. Loss: 4.207117080688477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d63236fcfec4e69bda2826298aa1521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231/250, Avg. Loss: 4.382949352264404\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6676541ab93f4d24ae78870e0517d4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/250, Avg. Loss: 2.176503896713257\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff559597a8f420b88390b78548d3038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/250, Avg. Loss: 1.1691149473190308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5cd7fa0367451bb4c339eb285c2dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234/250, Avg. Loss: 3.7209322452545166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406a1fc502d74fbc91e60c8496a5963c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235/250, Avg. Loss: 3.50103497505188\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57755aee27f747a59e235f0ca95855b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236/250, Avg. Loss: 0.3408688008785248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b6a77e2a444aba96587d05ef094877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/250, Avg. Loss: 0.11743297427892685\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0a67982486437a80530852f939d8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/250, Avg. Loss: 3.7690751552581787\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edcd77538cad488db187babe454c4bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/250, Avg. Loss: 0.6554116010665894\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9711a1e762a841128c34d593973207fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/250, Avg. Loss: 2.629408121109009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a08a1f96bda469daf396e720914c322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/250, Avg. Loss: 0.38053402304649353\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d93b5f6de942008ef61f7972a39d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242/250, Avg. Loss: 0.33245664834976196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c18ea338b848d4a64218eda112d7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243/250, Avg. Loss: 1.2047793865203857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16272296d8741c0afbd7c3c6a47e753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244/250, Avg. Loss: 1.5094127655029297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9913fc26759c4157827819d0a7bfadab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245/250, Avg. Loss: 1.6874754428863525\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c25c0120cc411e9e76effff2b23dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246/250, Avg. Loss: 3.6309592723846436\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173d0645637e4459b99e4e1b36af7936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 247/250, Avg. Loss: 1.8472270965576172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89db7ee4d7ff4405bb6ddd3e44195c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248/250, Avg. Loss: 0.3612089455127716\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541f1b4a268c4f2f897afad7e95496e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249/250, Avg. Loss: 1.3701015710830688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37478e4ff828487b9d334a1cdcd109fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/250, Avg. Loss: 0.982866644859314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x163af4340>]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Now, lets put it all together in the training loop.\n",
    "'''\n",
    "\n",
    "# check if gpu is available\n",
    "device = 'cpu' \n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "print(f\"Using '{device}' device\")\n",
    "\n",
    "################################################################\n",
    "#         TODO: YOUR CODE HERE    \n",
    "#\n",
    "# 1. Define the loss function, optimizer, and the BERT model. Use the model hyperparameters from the PDF.\n",
    "# 2. Specify the hyperparameters for training (e.g., learning rate, batch size, etc.)\n",
    "# 3. Initialize wandb for logging.\n",
    "#\n",
    "################################################################\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)  # Ignore padding tokens in the loss calculation\n",
    "batch_size = 4\n",
    "learning_rate = 1e-4\n",
    "epochs = 250\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "bert = BERT(vocab_size=tokenizer.get_vocab_size(), \n",
    "            hidden_size=128, \n",
    "            num_heads=4, \n",
    "            num_layers=2,\n",
    "        )\n",
    "\n",
    "optimizer = torch.optim.AdamW(bert.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "wandb.init(project=\"Homework3\", name=\"BERT_for_MLM_test2\")\n",
    "\n",
    "# Move model and loss function to the specified device\n",
    "bert.to(device)\n",
    "loss_fn.to(device)\n",
    "\n",
    "################################################################\n",
    "#         TODO: YOUR CODE HERE       #\n",
    "# THe training loop is where we train the model.              #\n",
    "#                                                             #\n",
    "# You should implement the following:                         #\n",
    "# 1. Load the input_ids, attention_mask, and labels to the    #\n",
    "#    device.                                                  #\n",
    "# 2. Zero the gradients of the optimizer.                     #\n",
    "# 3. Get the output from the BERT model.                      #\n",
    "# 4. Calculate the loss using the output and the labels.      #\n",
    "# 5. Backpropagate the loss.                                  #\n",
    "# 6. Update the optimizer.                                    #\n",
    "# 7. Log the loss to wandb.                                   #\n",
    "# 8. Save the model every epoch. Use separate files per epoch #\n",
    "#\n",
    "# HINT: The tokenizer can be helpful here.\n",
    "################################################################\n",
    "\n",
    "# Set the model to training mode\n",
    "bert.train()\n",
    "\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "# for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for i, (input_ids, attention_mask, labels) in enumerate(tqdm(dataloader, position=1, leave=True, desc=\"Step\")):\n",
    "        # try:\n",
    "        # Move input data to the device\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print(f\"Input ids shape: {input_ids.shape}\")\n",
    "        # print(f\"Labels shape: {labels.shape}\")\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, attn = bert(input_ids, attention_mask)  \n",
    "\n",
    "        # print(f\"Output: {outputs.shape}\")\n",
    "        # print(f\"Output shape: {outputs.view(-1, outputs.size(-1)).shape}\")\n",
    "        # print(f\"labels shape: {labels.view(-1).shape}\")\n",
    "\n",
    "        B, T, V = outputs.shape\n",
    "        outputs = outputs.view(B * T, V)\n",
    "        labels = labels.view(B * T)\n",
    "\n",
    "        # print(f\"output: {outputs.shape}\")\n",
    "        # print(f\"labels; {labels.shape}\")\n",
    "        \n",
    "        # Calculate the loss\n",
    "        # loss = loss_fn(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the loss\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # break\n",
    "\n",
    "        # if (i + 1) % 100 == 0:\n",
    "        wandb.log({\"Loss\": loss.item()}, step=epoch * len(dataloader) + i)\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Exception occurred: {e}\")\n",
    "        #     raise\n",
    "        \n",
    "    # Calculate average epoch loss\n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Avg. Loss: {avg_epoch_loss}\")\n",
    "\n",
    "    # Save the model (optional)\n",
    "torch.save(bert.state_dict(), f\"bert_epoch_{epoch + 1}.pt\")\n",
    "\n",
    "\n",
    "plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [CLS] i really like the book it was great and i loved reading it . [SEP]\n",
      "Noised:  [CLS] i really like the book it was great [MASK] i loved reading it . [SEP]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGuess:  \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([tokenizer\u001b[38;5;241m.\u001b[39mid_to_token(at_i) \u001b[38;5;28;01mfor\u001b[39;00m at_i \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ##\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     29\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI really like the book it was great and I loved reading it.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mnoise_and_predict_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 25\u001b[0m, in \u001b[0;36mnoise_and_predict_tokens\u001b[0;34m(query, tokenizer, model)\u001b[0m\n\u001b[1;32m     22\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m noise_inputs(ids, tokenizer\u001b[38;5;241m.\u001b[39mtoken_to_id(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoised: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([tokenizer\u001b[38;5;241m.\u001b[39mid_to_token(at_i) \u001b[38;5;28;01mfor\u001b[39;00m at_i \u001b[38;5;129;01min\u001b[39;00m inputs]))\n\u001b[0;32m---> 25\u001b[0m response, attns \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGuess:  \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([tokenizer\u001b[38;5;241m.\u001b[39mid_to_token(at_i) \u001b[38;5;28;01mfor\u001b[39;00m at_i \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ##\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/nut/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/nut/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 104\u001b[0m, in \u001b[0;36mBERT.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03marguments:\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03mx: The input token ids\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03mmask: The attention mask to apply to the input (see the collate function below)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m################################################################\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#                     TODO: YOUR CODE HERE                     #\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# print(f\"Input token ids: {x.shape}\")\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# print(f\"Mask shape: {mask.shape}\")\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# print(f\"embeddings shape: {embeddings.shape}\")\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# if mask is not None:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# mask_embeddings = mask\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# mask_embeddings = mask\u001b[39;00m\n\u001b[1;32m    120\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/nut/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/nut/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 42\u001b[0m, in \u001b[0;36mBertPositionalEmbedding.forward\u001b[0;34m(self, token_ids)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#                               TODO: YOUR CODE HERE                       #\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#                                                                          #\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#                                                                          #\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m     41\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m token_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m \u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# print(f\"Batch size: {batch_size}\")\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# print(f\"Seq len: {seq_len}\")\u001b[39;00m\n\u001b[1;32m     47\u001b[0m positions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(seq_len, device\u001b[38;5;241m=\u001b[39mtoken_ids\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now that we have trained the model, we can use it to predict the masked tokens.\n",
    "'''\n",
    "\n",
    "# Predict the masked tokens\n",
    "def noise_inputs(inputs, mask_token_id, mlm_probability=0.15):\n",
    "    inputs = deepcopy(inputs)\n",
    "    labels = [-100] * len(inputs)\n",
    "    masked_indices = np.random.choice(len(inputs), int(len(inputs) * mlm_probability), replace=False)\n",
    "    for i in masked_indices:\n",
    "        if inputs[i] not in [mask_token_id, 101, 102, 0]:\n",
    "            inputs[i] = mask_token_id\n",
    "            labels[i] = inputs[i]\n",
    "    return inputs, labels\n",
    "\n",
    "def noise_and_predict_tokens(query, tokenizer, model) -> str:\n",
    "    with torch.no_grad():\n",
    "        tokenized_input = tokenizer.encode(query)        \n",
    "        tokens = tokenized_input.tokens\n",
    "        print('Original:', ' '.join(tokens))\n",
    "        ids = np.array(tokenized_input.ids)\n",
    "        inputs, labels = noise_inputs(ids, tokenizer.token_to_id('[MASK]'))\n",
    "        print('Noised: ', ' '.join([tokenizer.id_to_token(at_i) for at_i in inputs]))\n",
    "        \n",
    "        response, attns = model(torch.from_numpy(inputs).to(device))\n",
    "        response = response.argmax(-1).squeeze(0).tolist()\n",
    "        print('Guess:  ', ' '.join([tokenizer.id_to_token(at_i) for at_i in response[1:-1]]).replace(' ##', ''))\n",
    "\n",
    "s = 'I really like the book it was great and I loved reading it.'\n",
    "noise_and_predict_tokens(s, tokenizer, bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.4: Save the Pre-Trained Model\n",
    "\n",
    "At this point, save the model's parameters in its `state_dict`. See pytorch's [documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for some guidance here. We'll be using this pre-trained model in later notebooks so once you save it, test that you can load it in another notebook (try `BERT_Inference.ipynb` to start) before moving on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################\n",
    "#                     TODO: YOUR CODE HERE                     #\n",
    "#\n",
    "# 1. Save the BERT model to a file\n",
    "#\n",
    "# NOTE: Before you close this notebook, verify you can load the model and \n",
    "#       use it to predict the masked tokens in the BERT_Inference notebook.\n",
    "#\n",
    "################################################################\n",
    "\n",
    "model = BERT(vocab_size=tokenizer.get_vocab_size(), \n",
    "            hidden_size=128, \n",
    "            num_heads=4, \n",
    "            num_layers=2,\n",
    "        )\n",
    "\n",
    "model_path = \"bert_epoch_1.pt\"\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "state_dict = torch.load(model_path)\n",
    "\n",
    "# Load the state dictionary into your BERT model\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.5 Convert the notebook to a script and submit to Great Lakes for final training\n",
    "\n",
    "Once you have your model debugged and can verify that it works on a small dataset (manual exploration in Part 4 will help), it's time to train it on more data and for a longer time period. To do this, we'll use the Great Lakes cluster at U-M which will give you access to a GPU that will make training run ~10x faster; this means more epochs and more data in the same amount of time so you get a better model. The Homework PDF has documentation on how use Great Lakes if you haven't seen it. The course account is limited to 4 hours of wallclock time and 16GB of memory, which were tuned specific for this assignment.\n",
    "\n",
    "Great Lakes supports interactive mode with Jupyter and running a script as a job. We **strongly** encourage the latter. To get a GPU, you'll need to submit a job to the cluster, which uses [SLURM for scheduling](https://arc.umich.edu/greatlakes/slurm-user-guide/). If you attempt to queue for an interactive job, you will have no control over when it starts, so you may end up having your notebook run for 4 hours from 3am to 7am and then it ends, at which point you have to get back in the queue. If you submit a job as a script (i.e., a .py file that runs the code in this notebook), it will run for the specified amount of time and save the BERT model without you having to interact with anything. \n",
    "\n",
    "SLURM and cluster scheduling is very common in some industries where there is a single cluster resource and people share it by submitting jobs to run so that no one can monopolize the system and that jobs can run in parallel. Given that Great Lakes will be useful to you in future assignments and projects, we strongly encourage you to learn how to use it effectively in this assignment.\n",
    "\n",
    "Depending on how you're working on this file, there's a few ways to directly convert the notebook to a file if you use [Jupyter or the command line](https://mljar.com/blog/convert-jupyter-notebook-python/) or [VSCode](https://stackoverflow.com/questions/64297272/best-way-to-convert-ipynb-to-py-in-vscode). Once you convert it, you'll modify the file some to change the epochs and text file as specified in the PDF. **We also strongly recommend having your script save the model at the end of every epoch.**  That way, if your script takes longer than 4 hours and gets killed, you still have the best saved model you could get based on the amount of training you could do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyObT2o6Mn3FXBXu/mG1x7SL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
